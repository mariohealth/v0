version: 2

sources:
  - name: bigquery-public-data
    description: bla
    database: bigquery-public-data
    schema: geo_us_boundaries

    tables:
      - name: zip_codes
        description: |
         Bla bla

  - name: mario-mrf-data
    description: files that I manually uploaded into Big Query
    database: mario-mrf-data
    schema: test_arnaud

    tables:
      - name: npidata_pfile_20050523-20250907
        description: downloaded from the CMS website and loaded in Big Query with the UI
      - name: cigna_nvidia_in_network_rates
        description: | 
          This BigQuery table is built from all the JSON files stored in mariohc-mrf-prod-usa/test_arnaud/cigna_nvidia_output_sharded
          The original file from Cigna was named "nvidia_2025-10-01_cigna-health-life-insurance-company_national-oap_in-network-rates.json"
          I ran a Python script on my own laptop to split that 1.34 GB zipped file into smaller JSONs
          I believe it applies to ALL Cigna National OAP plans for October 2025, not just for Nvidia
      - name: cigna_nvidia_provider_reference
        description: |
          This table is built from one of the first few rows of the file from above "nvidia_2025-10-01_cigna-health-life-insurance-company_national-oap_in-network-rates.json"
      - name: medicare_partB_utilization_raw
        description: |
          This table was downloaded from CMS and was first used to create a mapping from billing code to provider taxonomy
      - name: nucc_taxonomy_251_raw
        description: |
          This table is used to get the description of the provider taxonomy codes found in npidata_pfile_20050523-20250907
      - name: uhc_apple_in_network_rates
        description: |
          This table is built from all the JSON files stored in mariohc-mrf-prod-usa/test_arnaud/in-network-rates-sharded
          The original file from UHC was named "2025-09-01_United-HealthCare-Services--Inc-_Third-Party-Administrator_PP1-00_P3_in-network-rates.json"
          I ran a Python script on my own laptop to split that file into smaller JSONs
          I believe it applies to ALL UHC PP1-00_P3 plans for SEPTEMBER 2025, not just for Apple
          



#   Example of industry standard below:

# Define raw data sources loaded by Python scripts
#  - name: analytics
#    description: Raw data loaded by Python ingestion scripts
#    database: mario-health-prod  # Replace with your GCP project ID
#    schema: analytics
#
#    tables:
#      - name: raw_healthcare_prices
#        description: |
#          Raw pricing data from healthcare carriers (UHC, Aetna, etc.)
#          Loaded by: scripts/ingest_uhc_data.py and similar scripts
#
#        columns:
#          - name: cpt_code
#            description: CPT procedure code (5-digit)
#            tests:
#              - not_null
#
#          - name: procedure_name
#            description: Medical procedure name
#            tests:
#              - not_null
#
#          - name: provider_name
#            description: Healthcare provider/facility name
#            tests:
#              - not_null
#
#          - name: price
#            description: Procedure price in USD
#            tests:
#              - not_null
#
#          - name: carrier
#            description: Insurance carrier (UHC, Aetna, etc.)
#            tests:
#              - not_null
#              - accepted_values:
#                  values: ['UHC', 'Aetna', 'Cigna', 'Blue Cross', 'Humana']
#
#          - name: effective_date
#            description: Date when pricing becomes effective
#            tests:
#              - not_null
#
#        # Freshness check - warn if data is over 2 days old
#        loaded_at_field: effective_date
#        freshness:
#          warn_after: {count: 2, period: day}
#          error_after: {count: 7, period: day}
